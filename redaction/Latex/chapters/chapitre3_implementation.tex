\chapter{Implémentation Technique}
%\minitoc

\section{Introduction}

Ce chapitre détaille les aspects techniques de l'implémentation, incluant l'architecture logicielle, les adaptations spécifiques à chaque modèle, les défis rencontrés et leurs solutions. L'objectif est de fournir suffisamment de détails pour permettre la reproduction des expériences.

\section{Architecture Logicielle}

\subsection{Structure du Projet}

Le projet suit une architecture modulaire facilitant l'expérimentation :

\begin{verbatim}
nexar-collision-prediction/
  notebooks				             # EDA
  data/
    train/              # 1,500 videos MP4
    test/               # 1,344 videos MP4
    train.csv           # Annotations
    test.csv            # IDs test
  src/
    data/
      video_dataset.py    # PyTorch Dataset
      video_transforms.py # Augmentations
      dataloader.py
      feature_dataset.py
      transforms.py
    models/
      resnet_lstm.py
      efficientnet_gru.py
      i3d.py
      r2plus1d.py
      timesformer.py
      videomae.py
      lstm_features.py
    training/
      trainer.py          # Boucle d'entrainement
  scripts/
    train_resnet_lstm.py
    train_i3d.py
    ...
  features                # feature extrait  
  checkpoints/            # Modeles sauvegardes
  analysis/               # Graphiques, metriques
  submissions/            # Fichiers Kaggle
\end{verbatim}

\subsection{Dataset PyTorch}

La classe \texttt{VideoDataset} gère le chargement et prétraitement des vidéos :

\begin{lstlisting}[language=Python, caption=VideoDataset - Structure principale]
class VideoDataset(Dataset):
    def __init__(self, csv_path, video_dir, 
                 num_frames=16, frame_size=224, 
                 transform=None):
        self.df = pd.read_csv(csv_path)
        self.video_dir = video_dir
        self.num_frames = num_frames
        self.frame_size = frame_size
        self.transform = transform
    
    def __getitem__(self, idx):
        video_id = self.df.iloc[idx]['id']
        video_path = f"{self.video_dir}/{video_id}.mp4"
        
        # Extraire frames uniformement
        frames = self._extract_frames(video_path)
        
        # Appliquer transformations
        if self.transform:
            frames = self.transform(frames)
        
        label = self.df.iloc[idx]['target']
        return frames, label
\end{lstlisting}

\subsection{Extraction de Frames}

L'extraction utilise OpenCV pour échantillonner uniformément les frames :

\begin{lstlisting}[language=Python, caption=Extraction uniforme de frames]
def _extract_frames(self, video_path, num_frames=16):
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # Indices uniformement espaces
    indices = np.linspace(0, total_frames-1, 
                         num_frames, dtype=int)
    
    frames = []
    for idx in indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame = cv2.resize(frame, 
                              (self.frame_size, self.frame_size))
            frames.append(frame)
    
    cap.release()
    return np.array(frames)  # Shape: (T, H, W, 3)
\end{lstlisting}

\section{Implémentation des Modèles Hybrides}

\subsection{Pré-extraction de Features}

Pour optimiser l'entraînement des modèles CNN-RNN, nous séparons l'extraction de features :

\begin{lstlisting}[language=Python, caption=Script de pré-extraction]
# Phase 1: Extraire features ResNet50
backbone = models.resnet50(pretrained=True)
backbone.fc = nn.Identity()  # Retirer le classifieur
backbone.eval()
backbone.cuda()

features_dict = {}
for video_id, frames in tqdm(dataloader):
    with torch.no_grad():
        # frames: (B, T, C, H, W)
        B, T = frames.shape[:2]
        frames = frames.view(B*T, C, H, W)
        
        # Extraire features
        feats = backbone(frames.cuda())  # (B*T, 2048)
        feats = feats.view(B, T, -1)     # (B, T, 2048)
        
        features_dict[video_id] = feats.cpu()

# Sauvegarder
torch.save(features_dict, 'features_resnet50.pth')
\end{lstlisting}

Cette approche réduit drastiquement le temps d'entraînement (de 10h à 3h).

\subsection{ResNet-LSTM - Architecture}

\begin{lstlisting}[language=Python, caption=ResNet-LSTM Implementation]
class ResNetLSTM(nn.Module):
    def __init__(self, input_dim=2048, hidden_dim=512, 
                 num_layers=3, dropout=0.3):
        super().__init__()
        
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            dropout=dropout,
            batch_first=True
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        # x: (B, T, 2048) features pre-extraites
        lstm_out, (h_n, c_n) = self.lstm(x)
        
        # Prendre le dernier hidden state
        last_hidden = h_n[-1]  # (B, hidden_dim)
        
        # Classification
        output = self.classifier(last_hidden)
        return output.squeeze()
\end{lstlisting}

\subsection{EfficientNet-GRU - Architecture Bidirectionnelle}

\begin{lstlisting}[language=Python, caption=EfficientNet-GRU Implementation]
class EfficientNetGRU(nn.Module):
    def __init__(self, input_dim=1280, hidden_dim=512, 
                 num_layers=3, dropout=0.5, bidirectional=True):
        super().__init__()
        
        self.gru = nn.GRU(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            dropout=dropout,
            batch_first=True,
            bidirectional=bidirectional
        )
        
        # Adapter dimension si bidirectionnel
        gru_output_dim = hidden_dim * 2 if bidirectional else hidden_dim
        
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(gru_output_dim, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        # x: (B, T, 1280)
        gru_out, h_n = self.gru(x)
        
        if self.gru.bidirectional:
            # Concatener forward et backward
            h_forward = h_n[-2]
            h_backward = h_n[-1]
            last_hidden = torch.cat([h_forward, h_backward], dim=1)
        else:
            last_hidden = h_n[-1]
        
        output = self.classifier(last_hidden)
        return output.squeeze()
\end{lstlisting}

\section{Implémentation des CNN 3D}

\subsection{I3D (R3D-18) - Adaptation pour Classification Binaire}

\begin{lstlisting}[language=Python, caption=I3D avec R3D-18 backbone]
from torchvision.models.video import r3d_18, R3D_18_Weights

class I3D(nn.Module):
    def __init__(self, num_classes=2, pretrained=True, dropout=0.5):
        super().__init__()
        
        # Charger R3D-18 pre-entraine Kinetics-400
        if pretrained:
            weights = R3D_18_Weights.KINETICS400_V1
            self.backbone = r3d_18(weights=weights)
        else:
            self.backbone = r3d_18(weights=None)
        
        # Remplacer classifieur final
        in_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Sequential(
            nn.Dropout(p=dropout),
            nn.Linear(in_features, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(p=dropout),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        # x: (B, T, C, H, W)
        # R3D attend (B, C, T, H, W)
        x = x.permute(0, 2, 1, 3, 4)
        return self.backbone(x)
\end{lstlisting}

\textbf{Note importante} : La permutation des dimensions est cruciale. R3D attend l'ordre $(B, C, T, H, W)$ alors que notre dataset retourne $(B, T, C, H, W)$.

\subsection{R(2+1)D - Convolutions Factorisées}

\begin{lstlisting}[language=Python, caption=R(2+1)D Implementation]
from torchvision.models.video import r2plus1d_18

class R2Plus1D(nn.Module):
    def __init__(self, pretrained=True):
        super().__init__()
        
        if pretrained:
            self.backbone = r2plus1d_18(pretrained=True)
        else:
            self.backbone = r2plus1d_18(pretrained=False)
        
        # Adapter pour classification binaire
        in_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Linear(in_features, 1)
    
    def forward(self, x):
        x = x.permute(0, 2, 1, 3, 4)  # (B, C, T, H, W)
        logits = self.backbone(x)
        return torch.sigmoid(logits).squeeze()
\end{lstlisting}

\section{Implémentation des Vision Transformers}

\subsection{TimeSformer - Architecture from Scratch}

\begin{lstlisting}[language=Python, caption=TimeSformer Implementation]
from transformers import TimesformerModel, TimesformerConfig

class TimeSformer(nn.Module):
    def __init__(self, num_frames=8, img_size=224, 
                 patch_size=16, pretrained=False):
        super().__init__()
        
        if pretrained:
            # Charger poids pre-entraines (si disponibles)
            self.model = TimesformerModel.from_pretrained(
                'facebook/timesformer-base-finetuned-k400'
            )
        else:
            # Initialisation aleatoire
            config = TimesformerConfig(
                num_frames=num_frames,
                image_size=img_size,
                patch_size=patch_size,
                num_labels=1
            )
            self.model = TimesformerModel(config)
        
        # Classifieur
        hidden_size = self.model.config.hidden_size
        self.classifier = nn.Sequential(
            nn.LayerNorm(hidden_size),
            nn.Linear(hidden_size, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        # x: (B, T, C, H, W)
        outputs = self.model(x)
        
        # Pooling sur les tokens
        pooled = outputs.last_hidden_state.mean(dim=1)
        
        return self.classifier(pooled).squeeze()
\end{lstlisting}

\subsection{VideoMAE - Fine-tuning depuis Kinetics}

\begin{lstlisting}[language=Python, caption=VideoMAE Implementation]
from transformers import VideoMAEForVideoClassification

class VideoMAE(nn.Module):
    def __init__(self, num_frames=16, pretrained=True):
        super().__init__()
        
        if pretrained:
            # Charger VideoMAE pre-entraine
            self.model = VideoMAEForVideoClassification.from_pretrained(
                'MCG-NJU/videomae-base-finetuned-kinetics',
                num_labels=1,
                ignore_mismatched_sizes=True
            )
        else:
            raise ValueError("VideoMAE requires pretraining")
    
    def forward(self, x):
        # x: (B, T, C, H, W)
        outputs = self.model(x)
        logits = outputs.logits
        return torch.sigmoid(logits).squeeze()
\end{lstlisting}

\section{Boucle d'Entraînement}

\subsection{Trainer Générique}

\begin{lstlisting}[language=Python, caption=Boucle d'entraînement avec mixed precision]
from torch.cuda.amp import autocast, GradScaler

class Trainer:
    def __init__(self, model, train_loader, val_loader, 
                 criterion, optimizer, scheduler, device):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.criterion = criterion
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device
        self.scaler = GradScaler()  # Mixed precision
        
        self.best_ap = 0.0
        self.patience_counter = 0
    
    def train_epoch(self):
        self.model.train()
        total_loss = 0
        all_preds, all_labels = [], []
        
        for videos, labels in tqdm(self.train_loader):
            videos = videos.to(self.device)
            labels = labels.float().to(self.device)
            
            self.optimizer.zero_grad()
            
            # Mixed precision forward
            with autocast():
                outputs = self.model(videos)
                loss = self.criterion(outputs, labels)
            
            # Backward avec scaler
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
            
            total_loss += loss.item()
            all_preds.extend(outputs.detach().cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
        
        # Calculer metriques
        accuracy = accuracy_score(
            np.array(all_labels), 
            (np.array(all_preds) > 0.5).astype(int)
        )
        avg_loss = total_loss / len(self.train_loader)
        
        return avg_loss, accuracy
    
    def validate(self):
        self.model.eval()
        total_loss = 0
        all_preds, all_labels = [], []
        
        with torch.no_grad():
            for videos, labels in self.val_loader:
                videos = videos.to(self.device)
                labels = labels.float().to(self.device)
                
                outputs = self.model(videos)
                loss = self.criterion(outputs, labels)
                
                total_loss += loss.item()
                all_preds.extend(outputs.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # Metriques
        avg_loss = total_loss / len(self.val_loader)
        accuracy = accuracy_score(
            np.array(all_labels),
            (np.array(all_preds) > 0.5).astype(int)
        )
        ap = average_precision_score(
            np.array(all_labels),
            np.array(all_preds)
        )
        
        return avg_loss, accuracy, ap
\end{lstlisting}

\subsection{Early Stopping et Checkpointing}

\begin{lstlisting}[language=Python, caption=Early stopping implementation]
def train(self, num_epochs, patience=10):
    for epoch in range(num_epochs):
        # Train
        train_loss, train_acc = self.train_epoch()
        
        # Validate
        val_loss, val_acc, val_ap = self.validate()
        
        # Learning rate scheduling
        self.scheduler.step(val_ap)
        
        # Early stopping sur AP
        if val_ap > self.best_ap:
            self.best_ap = val_ap
            self.patience_counter = 0
            
            # Sauvegarder meilleur modele
            torch.save({
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'best_ap': self.best_ap,
                'val_acc': val_acc
            }, 'checkpoints/best_model.pth')
        else:
            self.patience_counter += 1
        
        # Stop si pas d'amelioration
        if self.patience_counter >= patience:
            print(f"Early stopping at epoch {epoch}")
            break
        
        print(f"Epoch {epoch}: "
              f"Train Loss={train_loss:.4f}, "
              f"Val AP={val_ap:.4f}")
\end{lstlisting}

\section{Défis Techniques et Solutions}

\subsection{Saturation Mémoire GPU}

\textbf{Problème} : Les modèles 3D et Transformers saturent rapidement la mémoire GPU (OOM errors).

\textbf{Solutions appliquées} :
\begin{itemize}
    \item Réduction de batch size (32 $\rightarrow$ 16 $\rightarrow$ 8)
    \item Réduction de résolution (224×224 $\rightarrow$ 160×160)
    \item Mixed precision training (FP16)
    \item Gradient accumulation pour simuler batch sizes plus grands
\end{itemize}

\subsection{Temps de Chargement Vidéos}

\textbf{Problème} : Chargement vidéo avec OpenCV est un goulot d'étranglement.

\textbf{Solutions} :
\begin{itemize}
    \item DataLoader avec \texttt{num\_workers > 0} pour parallélisme
    \item Pré-extraction features pour modèles hybrides
    \item Cache des frames en RAM quand possible
\end{itemize}

\subsection{Format de Tenseurs Inconsistant}

\textbf{Problème} : Différents modèles attendent différents ordres de dimensions.

\textbf{Solution} : Standardisation stricte :
\begin{itemize}
    \item Dataset retourne : $(B, T, C, H, W)$
    \item Permutation explicite dans le forward : $(B, C, T, H, W)$ pour modèles vidéo
\end{itemize}

\section{Tests et Validation}

\subsection{Tests Unitaires}

Chaque composant est testé indépendamment :

\begin{lstlisting}[language=Python]
# Test dataset
def test_video_dataset():
    dataset = VideoDataset('train.csv', 'train/', num_frames=8)
    video, label = dataset[0]
    assert video.shape == (8, 3, 224, 224)
    assert label in [0, 1]

# Test modele
def test_i3d_forward():
    model = I3D(pretrained=False)
    x = torch.randn(2, 8, 3, 160, 160)
    output = model(x)
    assert output.shape == (2, 2)  # (batch, num_classes)
\end{lstlisting}

\subsection{Validation sur Subset}

Avant l'entraînement complet, validation sur un petit subset (50 vidéos) pour détecter rapidement les bugs.

\section{Synthèse}

Ce chapitre a détaillé l'implémentation technique complète du projet :

\begin{itemize}
    \item Architecture logicielle modulaire et réutilisable
    \item Stratégie de pré-extraction pour optimiser les modèles hybrides
    \item Implémentations détaillées des 6 architectures
    \item Boucle d'entraînement avec mixed precision et early stopping
    \item Solutions aux défis techniques rencontrés (mémoire, performance, formats)
    \item Tests et validation pour assurer la robustesse
\end{itemize}
\begin{flushleft}
Le chapitre suivant présentera les résultats expérimentaux obtenus avec ces implémentations.
\end{flushleft}
