\chapter{Discussion et Recommandations}
%\minitoc

\section{Introduction}

Ce chapitre analyse en profondeur les résultats expérimentaux, identifie les patterns clés, et formule des recommandations pratiques pour le choix d'architecture selon les contraintes applicatives. Nous discutons également des limitations de notre étude et des perspectives de recherche future.

\section{Analyse Comparative Approfondie}

\subsection{Hiérarchie de Performance}

Les résultats établissent une hiérarchie claire selon la métrique Average Precision :

\begin{enumerate}
    \item \textbf{VideoMAE} (78,84\%) : Meilleur ranking, pré-training MAE sur Kinetics
    \item \textbf{I3D} (77,53\% / 71,2\% Kaggle) : Excellentes performances confirmées en test
    \item \textbf{R(2+1)D} (76,58\%) : Convolutions factorisées efficaces
    \item \textbf{EfficientNet-GRU} (74,95\%) : Meilleur hybride, accuracy maximale
    \item \textbf{ResNet-LSTM} (69,48\%) : Baseline solide
    \item \textbf{TimeSformer} (50,67\%) : Échec total sans pré-entraînement
\end{enumerate}

\subsection{Familles d'Architectures : Forces et Faiblesses}

\subsubsection{CNN-RNN Hybrides}

\textbf{Forces} :
\begin{itemize}
    \item Entraînement rapide via pré-extraction de features (3h vs 6-8h)
    \item Transfer learning efficace depuis ImageNet
    \item Interprétabilité : séparation claire spatial/temporel
    \item Meilleure accuracy brute (EfficientNet-GRU : 71\%)
    \item Moins d'overfitting que les modèles end-to-end
\end{itemize}

\textbf{Faiblesses} :
\begin{itemize}
    \item AP inférieur aux CNN 3D (74,95\% vs 77-78\%)
    \item Pas de modélisation conjointe spatio-temporelle
    \item Dépendance à un backbone 2D non optimisé pour vidéos
\end{itemize}

\textbf{Recommandation} : Privilégier pour prototypage rapide, déploiement avec contraintes computationnelles, ou quand l'interprétabilité est importante.

\subsubsection{CNN 3D}

\textbf{Forces} :
\begin{itemize}
    \item Excellent AP (77-78\%) grâce à la modélisation spatio-temporelle native
    \item Pré-entraînement Kinetics-400 très efficace
    \item Convolutions 3D capturent les motifs de mouvement locaux
    \item Performance confirmée sur test Kaggle (71,2\%)
\end{itemize}

\textbf{Faiblesses} :
\begin{itemize}
    \item Overfitting sévère sur dataset modeste (gap 30\%)
    \item Coût computationnel élevé (6h entraînement, 33M params)
    \item Besoin de réduire résolution (160×160) pour tenir en mémoire
\end{itemize}

\textbf{Recommandation} : Choisir I3D pour maximiser la performance de prédiction en production. Appliquer régularisation forte (dropout, data augmentation, early stopping) pour limiter l'overfitting.

\subsubsection{Vision Transformers}

\textbf{Forces} :
\begin{itemize}
    \item Meilleur AP absolu (VideoMAE : 78,84\%)
    \item Attention spatio-temporelle flexible
    \item Potentiel d'amélioration via scaling (plus de données/params)
\end{itemize}

\textbf{Faiblesses} :
\begin{itemize}
    \item \textbf{Pré-entraînement vidéo obligatoire} (échec total sinon : 50,67\%)
    \item Overfitting extrême (gap 31\%)
    \item Très coûteux (8h, 86-120M params)
    \item Difficile à déployer (latence, mémoire)
\end{itemize}

\textbf{Recommandation} : Utiliser uniquement avec pré-entraînement Kinetics. Privilégier pour recherche ou si ressources computationnelles illimitées. \textbf{Ne jamais entraîner from scratch}.

\section{Le Rôle Critique du Pré-entraînement}

\subsection{Observations Empiriques}

Notre expérience avec TimeSformer démontre de manière frappante l'importance du pré-entraînement :

\begin{table}[H]
\centering
\caption{Impact du pré-entraînement (Vision Transformers)}
\begin{tabular}{lccc}
\toprule
\textbf{Condition} & \textbf{Val Acc} & \textbf{Val AP} & \textbf{Convergence} \\
\midrule
Sans pré-training & 50,67\% & 50,67\% & Non \\
Avec pré-training Kinetics & 68,00\% & 78,84\% & Oui \\
\midrule
\textbf{Gain} & \textbf{+17,33\%} & \textbf{+28,17\%} & - \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Explication Théorique}

\textbf{Pourquoi les Transformers échouent sans pré-entraînement ?}

\begin{enumerate}
    \item \textbf{Biais inductifs faibles} : Contrairement aux CNN (localité, translation), les Transformers n'ont pas de biais intégrés. Ils doivent \textit{tout} apprendre des données.
    
    \item \textbf{Nombre de paramètres élevé} : 120M paramètres pour 1,200 vidéos = 100,000 params/vidéo. Ratio catastrophique.
    
    \item \textbf{Optimization landscape complexe} : Les Transformers ont un landscape d'optimisation non-convexe difficile à naviguer avec peu de données.
    
    \item \textbf{Sur-paramétrisation nécessaire} : Les Transformers nécessitent une sur-paramétrisation massive pour bien généraliser, ce qui est incompatible avec les petits datasets.
\end{enumerate}

\textbf{Le pré-entraînement} fournit une initialisation dans une région favorable de l'espace des paramètres, permettant au fine-tuning de converger efficacement.

\subsection{Comparaison des Régimes de Pré-entraînement}

\begin{table}[H]
\centering
\caption{Efficacité du pré-entraînement selon la source}
\begin{tabular}{lcc}
\toprule
\textbf{Source} & \textbf{Type} & \textbf{AP Moyen} \\
\midrule
Kinetics-400 (vidéos) & 3D CNN & 77,06\% \\
Kinetics-400 (vidéos) & Transformers & 78,84\% \\
ImageNet (images) & 2D CNN + RNN & 72,22\% \\
Aucun & Transformers & 50,67\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Conclusion} : Le pré-entraînement sur \textit{vidéos} (Kinetics) est significativement plus efficace que sur \textit{images} (ImageNet) pour les architectures vidéo complètes.

\section{Compromis Performance vs Complexité}

\subsection{Analyse Multi-critères}

\begin{table}[H]
\centering
\caption{Évaluation multi-critères des architectures}
\begin{tabular}{lccccc}
\toprule
\textbf{Modèle} & \textbf{AP} & \textbf{Acc} & \textbf{Temps} & \textbf{Params} & \textbf{Score} \\
\midrule
EfficientNet-GRU & 74,95\% & 71\% & 3h & 1,6M & $\star\star\star\star\star$ \\
I3D & 77,53\% & 70\% & 6h & 33M & $\star\star\star\star$ \\
VideoMAE & 78,84\% & 68\% & 7h & 86M & $\star\star\star$ \\
R(2+1)D & 76,58\% & 68,67\% & 5h & 32M & $\star\star\star\star$ \\
ResNet-LSTM & 69,48\% & 67,33\% & 3h & 2,9M & $\star\star\star$ \\
TimeSformer & 50,67\% & 50,67\% & 8h & 120M & $\star$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Score} basé sur : Performance (50\%), Vitesse (25\%), Efficacité paramétrique (25\%)

\subsection{Recommandations par Cas d'Usage}

\subsubsection{Cas 1 : Performance Maximale (Recherche, Benchmark)}

\textbf{Choix recommandé} : \textbf{VideoMAE} ou \textbf{I3D}

\textbf{Justification} :
\begin{itemize}
    \item AP maximal (78,84\% / 77,53\%)
    \item Validation Kaggle confirme robustesse (71,2\%)
    \item État de l'art actuel en reconnaissance d'actions
\end{itemize}

\textbf{Contraintes acceptées} :
\begin{itemize}
    \item Coût computationnel élevé (6-8h, GPU haute performance)
    \item Overfitting nécessitant régularisation forte
    \item Latence d'inférence élevée ($>$100ms/vidéo)
\end{itemize}

\subsubsection{Cas 2 : Déploiement Temps Réel (ADAS, Production)}

\textbf{Choix recommandé} : \textbf{EfficientNet-GRU}

\textbf{Justification} :
\begin{itemize}
    \item Meilleure accuracy (71\%) pour décisions critiques
    \item Entraînement rapide (3h) facilitant itérations
    \item Backbone léger (5,3M) déployable sur edge devices
    \item Latence d'inférence faible ($<$50ms/vidéo)
\end{itemize}

\textbf{Trade-off acceptable} : -3,89\% AP vs I3D (74,95\% vs 77,53\%)

\subsubsection{Cas 3 : Prototypage Rapide (Exploration, POC)}

\textbf{Choix recommandé} : \textbf{ResNet-LSTM}

\textbf{Justification} :
\begin{itemize}
    \item Implémentation simple et standardisée
    \item Entraînement rapide (3h)
    \item Baseline solide (69,48\% AP)
    \item Interprétabilité pour debugging
\end{itemize}

\subsubsection{Cas 4 : Contraintes Mémoire Extrêmes}

\textbf{Choix recommandé} : \textbf{R(2+1)D}

\textbf{Justification} :
\begin{itemize}
    \item Convolutions factorisées réduisent mémoire
    \item Performance proche de I3D (76,58\% vs 77,53\%)
    \item Moins d'overfitting que I3D
\end{itemize}

\section{Limitations de l'Étude}

\subsection{Taille du Dataset}

\textbf{Limitation} : 1,500 vidéos d'entraînement est modeste pour le deep learning moderne.

\textbf{Conséquences observées} :
\begin{itemize}
    \item Overfitting généralisé (gaps 15-31\%)
    \item Impossibilité d'entraîner Transformers from scratch
    \item Limitation de la généralisation
\end{itemize}

\textbf{Perspectives} : Augmenter le dataset via scraping supplémentaire ou génération synthétique (simulation).

\subsection{Horizons Temporels Limités}

\textbf{Limitation} : Évaluation uniquement à 0,5-1,5s avant collision.

\textbf{Manque} : Pas d'évaluation à horizons plus longs (2-3s) qui seraient plus exploitables en conditions réelles.

\subsection{Absence d'Interprétabilité}

\textbf{Limitation} : Aucune analyse d'interprétabilité (GradCAM, attention maps).

\textbf{Impact} : Impossibilité de comprendre \textit{quels} features visuels le modèle utilise (objets ? mouvements ? layout spatial ?).

\textbf{Perspectives} : Études futures avec visualisations d'attention et ablations spatiales.

\subsection{Évaluation sur Dataset Unique}

\textbf{Limitation} : Évaluation uniquement sur Nexar.

\textbf{Risque} : Biais spécifiques au dataset (ex: caractéristiques des dashcams Nexar).

\textbf{Perspectives} : Validation croisée sur d'autres datasets (A3D, PREVENTION, DADA-2000).

\section{Perspectives de Recherche}

\begin{enumerate}
    \item \textbf{Optimisation anti-overfitting} :
    \begin{itemize}
        \item Freeze partiel des couches basses
        \item Augmentation de données avancée (mixup, cutmix)
        \item Régularisation adaptative (DropBlock, Stochastic Depth)
    \end{itemize}
    
    \item \textbf{Ensemble methods} :
    \begin{itemize}
        \item Combiner I3D + EfficientNet-GRU via averaging pondéré
        \item Stacking avec meta-learner
        \item Espérance : +2-3\% AP
    \end{itemize}
    
    \item \textbf{Interprétabilité} :
    \begin{itemize}
        \item GradCAM++ pour localisation spatio-temporelle
        \item Attention maps pour Transformers
        \item Ablations pour identifier features critiques
    \end{itemize}
    
    \item \textbf{Architectures hybrides avancées} :
    \begin{itemize}
        \item CNN 3D + Transformer (capture local + global)
        \item Two-Stream avec optical flow
        \item Graph Neural Networks pour interactions objets
    \end{itemize}
    
    \item \textbf{Pré-entraînement self-supervised} :
    \begin{itemize}
        \item Masked Video Modeling (MVM) sur dashcam non-annotées
        \item Contrastive learning vidéo
        \item Pseudo-labeling sur dashcam YouTube
    \end{itemize}
    
    \item \textbf{Multi-tâches} :
    \begin{itemize}
        \item Prédiction collision + localisation temporelle
        \item Détection objets + prédiction collision (joint)
        \item Estimation Time-to-Collision continue
    \end{itemize}
    
    \item \textbf{Déploiement temps réel} :
    \begin{itemize}
        \item Quantization (INT8) pour edge devices
        \item Pruning et distillation de connaissances
        \item Architectures efficient (MobileNet-based)
    \end{itemize}
    
    \item \textbf{Généralisation multi-domaines} :
    \begin{itemize}
        \item Domain adaptation (Nexar $\rightarrow$ autres dashcams)
        \item Few-shot learning pour nouvelles conditions
        \item Robustness à adversarial attacks
    \end{itemize}
    
    \item \textbf{Intégration système ADAS} :
    \begin{itemize}
        \item Fusion capteurs (caméra + radar + lidar)
        \item Prédiction trajectoires + collision
        \item Système décisionnel complet (freinage, alerte)
    \end{itemize}
\end{enumerate}

\section{Recommandations Finales}

Sur la base de cette étude comparative exhaustive, nous formulons les recommandations suivantes :

\begin{enumerate}
    \item \textbf{Pour maximiser la performance} : Utiliser I3D avec pré-entraînement Kinetics-400, appliquer early stopping strict, et envisager ensembling.
    
    \item \textbf{Pour le déploiement industriel} : Privilégier EfficientNet-GRU pour le compromis optimal performance/complexité. Optimiser via quantization si nécessaire.
    
    \item \textbf{Pour les Transformers} : \textbf{Ne jamais} entraîner from scratch. Toujours utiliser pré-entraînement Kinetics. Accepter les coûts computationnels élevés.
    
    \item \textbf{Pour combattre l'overfitting} : Combiner dropout fort, early stopping, data augmentation, et considérer freeze partiel du backbone.
    
    \item \textbf{Pour la recherche future} : Investir dans l'interprétabilité, le pré-entraînement self-supervised sur dashcam, et les architectures hybrides.
\end{enumerate}

\section{Synthèse}

Ce chapitre a analysé en profondeur les résultats expérimentaux, établi des recommandations pratiques par cas d'usage, identifié les limitations, et proposé des perspectives de recherche future. Les enseignements clés sont :

\begin{itemize}
    \item I3D et VideoMAE obtiennent les meilleures performances mais au prix d'un coût computationnel élevé
    \item EfficientNet-GRU offre le meilleur compromis pour le déploiement réel
    \item Le pré-entraînement vidéo est absolument critique pour les Transformers
    \item L'overfitting est un défi majeur nécessitant régularisation multi-facettes
    \item De nombreuses pistes d'amélioration existent (ensemble, interprétabilité, architectures hybrides)
\end{itemize}
