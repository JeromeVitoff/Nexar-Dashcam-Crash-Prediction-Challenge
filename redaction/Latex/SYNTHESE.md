# SYNTH√àSE DU M√âMOIRE
## Analyse Comparative d'Architectures Deep Learning pour la Pr√©diction de Collisions Routi√®res

**Auteur:** VITOFFODVI Adjimon  
**Encadrant:** J√©r√¥me PASQUET  
**Universit√©:** Paul Val√©ry Montpellier 3  
**Date:** Janvier 2026

---

## üìã STRUCTURE COMPL√àTE

### Pages Pr√©liminaires
- ‚úÖ Page de garde (front_page.tex)
- ‚úÖ Remerciements
- ‚úÖ R√©sum√© (FR + EN)
- ‚úÖ Table des mati√®res
- ‚úÖ Liste des figures
- ‚úÖ Liste des tableaux

### Corps du M√©moire

#### Introduction (~10 pages)
- Contexte et probl√©matique (1,3M d√©c√®s/an)
- D√©fis scientifiques et techniques
- √âtat de l'art et approches existantes
- Objectifs du m√©moire
- Cadre du projet (Challenge Kaggle Nexar)
- Contributions
- Organisation du m√©moire

#### Chapitre 1 : √âtat de l'Art (~20 pages)
1. Fondements th√©oriques du Deep Learning pour la vid√©o
2. Architectures hybrides CNN-RNN
   - ResNet-LSTM
   - EfficientNet-GRU
3. CNN 3D
   - I3D (Inflated 3D ConvNet)
   - R(2+1)D (Convolutions factoris√©es)
4. Vision Transformers
   - TimeSformer
   - VideoMAE
5. Travaux connexes sur la pr√©diction de collisions

#### Chapitre 2 : M√©thodologie (~15 pages)
1. Dataset Nexar (1,500 train + 1,344 test)
2. Protocole exp√©rimental
3. M√©triques d'√©valuation (Accuracy, AP, mAP)
4. Configuration sp√©cifique des 6 mod√®les
5. Strat√©gies d'optimisation
6. Reproductibilit√©

#### Chapitre 3 : Impl√©mentation Technique (~15 pages)
1. Architecture logicielle
2. Impl√©mentation mod√®les hybrides (pr√©-extraction features)
3. Impl√©mentation CNN 3D
4. Impl√©mentation Vision Transformers
5. Boucle d'entra√Ænement (mixed precision, early stopping)
6. D√©fis techniques et solutions

#### Chapitre 4 : R√©sultats Exp√©rimentaux (~20 pages)
1. Mod√®les hybrides CNN-RNN
   - ResNet-LSTM : 67,33% acc, 69,48% AP
   - EfficientNet-GRU : **71% acc**, 74,95% AP
2. CNN 3D
   - I3D : 70% acc, **77,53% AP**, **71,2% Kaggle**
   - R(2+1)D : 68,67% acc, 76,58% AP
3. Vision Transformers
   - TimeSformer : 50,67% (√©chec total sans pr√©-training)
   - VideoMAE : 68% acc, **78,84% AP** (meilleur)
4. Tableau comparatif global
5. Analyse des patterns observ√©s
6. Validation Kaggle

#### Chapitre 5 : Discussion (~15 pages)
1. Analyse comparative approfondie
2. Le r√¥le critique du pr√©-entra√Ænement (+28% AP)
3. Compromis performance vs complexit√©
4. Recommandations par cas d'usage
5. Limitations de l'√©tude
6. Perspectives de recherche

#### Conclusion (~5 pages)
- Rappel des objectifs
- Contributions principales
- Enseignements cl√©s
- Limitations et travaux futurs
- Impact et applications
- Recommandations finales

### Pages Finales
- ‚úÖ Bibliographie (50+ r√©f√©rences)

---

## üìä R√âSULTATS CL√âS

### Classement Final par Average Precision

| Rang | Mod√®le | AP | Accuracy | Famille |
|------|--------|-----|----------|---------|
| ü•á | VideoMAE | **78,84%** | 68,00% | Transformer |
| ü•à | I3D | **77,53%** | 70,00% | 3D CNN |
| ü•â | R(2+1)D | 76,58% | 68,67% | 3D CNN |
| 4 | EfficientNet-GRU | 74,95% | **71,00%** | CNN-RNN |
| 5 | ResNet-LSTM | 69,48% | 67,33% | CNN-RNN |
| 6 | TimeSformer | 50,67% | 50,67% | Transformer |

### Score Kaggle (I3D)
- **Public Leaderboard:** 66,9%
- **Private Leaderboard:** **71,2%** ‚úÖ

---

## üéØ CONTRIBUTIONS MAJEURES

1. **Comparaison exp√©rimentale exhaustive** de 6 architectures repr√©sentatives

2. **D√©monstration empirique** de l'importance du pr√©-entra√Ænement vid√©o :
   - TimeSformer (sans) : 50,67% AP
   - VideoMAE (avec) : 78,84% AP
   - **Gain : +28,17%**

3. **Identification de I3D** comme architecture optimale (77,53% AP, 71,2% Kaggle)

4. **EfficientNet-GRU** comme meilleur compromis vitesse/performance

5. **M√©thodologie reproductible** avec optimisations document√©es

---

## üí° ENSEIGNEMENTS PRINCIPAUX

### ‚úÖ √Ä FAIRE
- **Toujours** pr√©-entra√Æner les Transformers sur vid√©os (Kinetics minimum)
- Utiliser I3D pour maximiser la performance de pr√©diction
- Choisir EfficientNet-GRU pour d√©ploiement temps r√©el
- Appliquer early stopping strict (patience 10)
- Combiner dropout, data augmentation, weight decay

### ‚ùå √Ä √âVITER
- **JAMAIS** entra√Æner Transformers from scratch (√©chec garanti)
- N√©gliger le pr√©-entra√Ænement vid√©o pour CNN 3D
- Ignorer l'overfitting (gaps jusqu'√† 31%)
- Sous-estimer l'importance du pr√©-entra√Ænement

---

## üìö NOMBRE DE PAGES ESTIM√â

- **Pages pr√©liminaires :** ~10 pages
- **Introduction :** ~10 pages
- **Chapitre 1 (√âtat de l'art) :** ~20 pages
- **Chapitre 2 (M√©thodologie) :** ~15 pages
- **Chapitre 3 (Impl√©mentation) :** ~15 pages
- **Chapitre 4 (R√©sultats) :** ~20 pages
- **Chapitre 5 (Discussion) :** ~15 pages
- **Conclusion :** ~5 pages
- **Bibliographie :** ~5 pages

**TOTAL ESTIM√â : ~115 pages**

---

## üé® GRAPHIQUES √Ä INCLURE

### Cr√©√©s par le script `generate_figures.py` :

1. ‚úÖ **ap_comparison.pdf** - Barplot AP par mod√®le
2. ‚úÖ **accuracy_comparison.pdf** - Barplot Accuracy par mod√®le
3. ‚úÖ **accuracy_vs_ap.pdf** - Scatter Accuracy vs AP
4. ‚úÖ **overfitting_analysis.pdf** - Gap Train-Val
5. ‚úÖ **training_time.pdf** - Temps d'entra√Ænement
6. ‚úÖ **parameters_count.pdf** - Nombre de param√®tres
7. ‚úÖ **pretraining_impact.pdf** - TimeSformer vs VideoMAE
8. ‚úÖ **family_comparison.pdf** - Performance par famille

### √Ä cr√©er manuellement (optionnel) :

- Courbes d'apprentissage par mod√®le (loss/accuracy vs epochs)
- Matrices de confusion
- Courbes Precision-Recall
- Exemples visuels de pr√©dictions

---

## üöÄ PROCHAINES √âTAPES

### √âtape 1 : G√©n√©rer les graphiques
```bash
cd memoire/
python3 generate_figures.py
```

### √âtape 2 : Compiler le m√©moire
```bash
make
# ou
pdflatex manuscript.tex
bibtex manuscript
pdflatex manuscript.tex
pdflatex manuscript.tex
```

### √âtape 3 : V√©rifier le PDF
```bash
make view
# ou ouvrir manuscript.pdf
```

### √âtape 4 : Ajustements finaux
- Relecture compl√®te
- V√©rification des r√©f√©rences
- Correction orthographique
- Ajout de figures suppl√©mentaires si n√©cessaire

---

## üìû CONTACTS

**Encadrant :** J√©r√¥me PASQUET  
**Universit√© :** Paul Val√©ry Montpellier 3  
**Formation :** Master 2 MIASHS

---

**Date de cr√©ation :** Janvier 2026  
**Derni√®re mise √† jour :** 19 janvier 2026
