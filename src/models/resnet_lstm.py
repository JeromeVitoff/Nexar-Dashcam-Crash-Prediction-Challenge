"""
ResNet-LSTM pour Classification de Vid√©os
==========================================

Mod√®le baseline combinant :
- ResNet50 (pr√©-entra√Æn√© ImageNet) pour l'extraction de features spatiales
- LSTM pour la mod√©lisation temporelle
- Fully Connected pour la classification

Architecture :
    Video (T, 3, H, W) ‚Üí ResNet50 ‚Üí Features (T, 2048)
                      ‚Üí LSTM ‚Üí Hidden (256)
                      ‚Üí FC ‚Üí Logits (2)

Ce mod√®le sert de baseline pour comparer avec des architectures
plus avanc√©es (3D CNN, Transformers).

Auteur: Jerome
Date: Octobre 2025
Exp√©rience: exp_001 (baseline)
"""

import torch
import torch.nn as nn
import torchvision.models as models
from typing import Tuple, Dict


class ResNetLSTM(nn.Module):
    """
    Mod√®le ResNet-LSTM pour classification de vid√©os de dashcam.
    
    Ce mod√®le combine un CNN pr√©-entra√Æn√© (ResNet50) pour extraire
    les features spatiales de chaque frame, avec un LSTM pour capturer
    les d√©pendances temporelles entre les frames.
    
    Args:
        num_classes (int): Nombre de classes (2 pour collision/normal)
        lstm_hidden_size (int): Taille de l'√©tat cach√© du LSTM
        lstm_num_layers (int): Nombre de couches LSTM
        dropout (float): Dropout rate pour r√©gularisation
        freeze_backbone (bool): Si True, freeze les poids de ResNet
        pretrained (bool): Utiliser les poids pr√©-entra√Æn√©s ImageNet
        
    Input Shape:
        (batch_size, num_frames, 3, 224, 224)
        
    Output Shape:
        (batch_size, num_classes)
        
    Example:
        >>> model = ResNetLSTM(num_classes=2, lstm_hidden_size=256)
        >>> x = torch.randn(8, 16, 3, 224, 224)  # batch=8, frames=16
        >>> output = model(x)
        >>> print(output.shape)  # (8, 2)
    """
    
    def __init__(
        self,
        num_classes: int = 2,
        lstm_hidden_size: int = 256,
        lstm_num_layers: int = 2,
        dropout: float = 0.3,
        freeze_backbone: bool = False,
        pretrained: bool = True
    ):
        super(ResNetLSTM, self).__init__()
        
        self.num_classes = num_classes
        self.lstm_hidden_size = lstm_hidden_size
        self.lstm_num_layers = lstm_num_layers
        self.dropout = dropout
        self._is_backbone_frozen = freeze_backbone
        
        # 1. Backbone CNN : ResNet50 pr√©-entra√Æn√©
        resnet = models.resnet50(pretrained=pretrained)
        
        # Extraire toutes les couches sauf la derni√®re FC
        # ResNet50 output: 2048-dimensional feature vector
        modules = list(resnet.children())[:-1]  # Enlever avgpool et fc
        self.backbone = nn.Sequential(*modules)
        
        # Taille des features extraites par ResNet50
        self.feature_dim = 2048
        
        # Freeze le backbone si demand√©
        if self._is_backbone_frozen:
            for param in self.backbone.parameters():
                param.requires_grad = False
            print("   üîí Backbone ResNet50 freez√© (pas d'entra√Ænement)")
        else:
            print("   üîì Backbone ResNet50 entra√Ænable (fine-tuning)")
        
        # 2. LSTM pour mod√©liser la s√©quence temporelle
        self.lstm = nn.LSTM(
            input_size=self.feature_dim,
            hidden_size=lstm_hidden_size,
            num_layers=lstm_num_layers,
            batch_first=True,
            dropout=dropout if lstm_num_layers > 1 else 0,
            bidirectional=False
        )
        
        # 3. Couche de classification
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(lstm_hidden_size, lstm_hidden_size // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(lstm_hidden_size // 2, num_classes)
        )
        
        # Initialisation des poids du classifier
        self._initialize_weights()
        
        print(f"‚úÖ ResNetLSTM initialis√©:")
        print(f"   ‚Ä¢ Backbone: ResNet50 (pretrained={pretrained})")
        print(f"   ‚Ä¢ Feature dim: {self.feature_dim}")
        print(f"   ‚Ä¢ LSTM: {lstm_num_layers} layers, hidden={lstm_hidden_size}")
        print(f"   ‚Ä¢ Dropout: {dropout}")
        print(f"   ‚Ä¢ Num classes: {num_classes}")
    
    def _initialize_weights(self):
        """Initialise les poids du classifier."""
        for m in self.classifier.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass.
        
        Args:
            x (torch.Tensor): Input de shape (batch, num_frames, 3, H, W)
            
        Returns:
            torch.Tensor: Logits de shape (batch, num_classes)
        """
        batch_size, num_frames, C, H, W = x.shape
        
        # 1. Extraire les features pour chaque frame
        # Reshape: (batch * num_frames, 3, H, W)
        x = x.view(batch_size * num_frames, C, H, W)
        
        # Forward √† travers le backbone CNN
        # Output: (batch * num_frames, feature_dim, 1, 1)
        features = self.backbone(x)
        
        # Flatten: (batch * num_frames, feature_dim)
        features = features.view(batch_size * num_frames, -1)
        
        # Reshape: (batch, num_frames, feature_dim)
        features = features.view(batch_size, num_frames, -1)
        
        # 2. Passer √† travers le LSTM
        # lstm_out: (batch, num_frames, lstm_hidden_size)
        # h_n: (num_layers, batch, lstm_hidden_size)
        lstm_out, (h_n, c_n) = self.lstm(features)
        
        # Prendre la sortie du dernier timestep
        # last_output: (batch, lstm_hidden_size)
        last_output = lstm_out[:, -1, :]
        
        # Alternativement, on peut utiliser h_n[-1] (dernier hidden state)
        # last_output = h_n[-1]
        
        # 3. Classification
        # logits: (batch, num_classes)
        logits = self.classifier(last_output)
        
        return logits
    
    def get_num_params(self) -> Tuple[int, int]:
        """
        Retourne le nombre de param√®tres du mod√®le.
        
        Returns:
            Tuple[int, int]: (total_params, trainable_params)
        """
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        return total_params, trainable_params
    
    def freeze_backbone(self):
        """Freeze les poids du backbone ResNet."""
        for param in self.backbone.parameters():
            param.requires_grad = False
        self._is_backbone_frozen = True
        print("üîí Backbone freez√©")
    
    def unfreeze_backbone(self):
        """Unfreeze les poids du backbone ResNet."""
        for param in self.backbone.parameters():
            param.requires_grad = True
        self._is_backbone_frozen = False
        print("üîì Backbone unfreez√©")
    
    def unfreeze_last_n_blocks(self, n: int = 1):
        """
        Unfreeze les n derniers blocs de ResNet pour fine-tuning progressif.
        
        ResNet50 a 4 blocs (layer1, layer2, layer3, layer4).
        
        Args:
            n (int): Nombre de blocs √† unfreeze (1-4)
        """
        # D'abord, freeze tout
        self.freeze_backbone()
        
        # Ensuite, unfreeze les derniers blocs
        blocks = [
            self.backbone[-4],  # layer4
            self.backbone[-5],  # layer3
            self.backbone[-6],  # layer2
            self.backbone[-7]   # layer1
        ]
        
        for i in range(min(n, len(blocks))):
            for param in blocks[i].parameters():
                param.requires_grad = True
        
        print(f"üîì Derniers {n} bloc(s) de ResNet unfreez√©s")


def count_parameters(model: nn.Module) -> Dict[str, int]:
    """
    Compte les param√®tres d'un mod√®le de mani√®re d√©taill√©e.
    
    Args:
        model (nn.Module): Mod√®le PyTorch
        
    Returns:
        Dict avec statistiques d√©taill√©es
    """
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    frozen = total - trainable
    
    return {
        'total': total,
        'trainable': trainable,
        'frozen': frozen,
        'trainable_percent': (trainable / total * 100) if total > 0 else 0
    }


def test_model():
    """
    Fonction de test pour v√©rifier le mod√®le.
    
    Usage:
        python -c "from resnet_lstm import test_model; test_model()"
    """
    print("üß™ TEST DU MOD√àLE ResNet-LSTM\n")
    print("="*70)
    
    # 1. Cr√©er le mod√®le
    print("\n1Ô∏è‚É£ Cr√©ation du mod√®le...")
    model = ResNetLSTM(
        num_classes=2,
        lstm_hidden_size=256,
        lstm_num_layers=2,
        dropout=0.3,
        freeze_backbone=False,
        pretrained=True
    )
    
    # 2. Compter les param√®tres
    print("\n2Ô∏è‚É£ Statistiques du mod√®le:")
    params = count_parameters(model)
    print(f"   ‚Ä¢ Total param√®tres: {params['total']:,}")
    print(f"   ‚Ä¢ Param√®tres entra√Ænables: {params['trainable']:,}")
    print(f"   ‚Ä¢ Param√®tres freez√©s: {params['frozen']:,}")
    print(f"   ‚Ä¢ % entra√Ænables: {params['trainable_percent']:.1f}%")
    
    # 3. Test forward pass
    print("\n3Ô∏è‚É£ Test du forward pass...")
    
    # Cr√©er un batch de test
    batch_size = 4
    num_frames = 16
    x = torch.randn(batch_size, num_frames, 3, 224, 224)
    print(f"   ‚Ä¢ Input shape: {x.shape}")
    
    # Forward
    model.eval()
    with torch.no_grad():
        output = model(x)
    
    print(f"   ‚Ä¢ Output shape: {output.shape}")
    print(f"   ‚Ä¢ Output range: [{output.min():.3f}, {output.max():.3f}]")
    
    # 4. Test avec diff√©rentes configurations
    print("\n4Ô∏è‚É£ Test avec diff√©rentes configurations...")
    
    configs = [
        {'num_frames': 8, 'batch_size': 8},
        {'num_frames': 16, 'batch_size': 4},
        {'num_frames': 32, 'batch_size': 2},
    ]
    
    for config in configs:
        x = torch.randn(config['batch_size'], config['num_frames'], 3, 224, 224)
        with torch.no_grad():
            output = model(x)
        print(f"   ‚úì {config['num_frames']} frames, batch={config['batch_size']}: "
              f"output shape {output.shape}")
    
    # 5. Test freeze/unfreeze
    print("\n5Ô∏è‚É£ Test freeze/unfreeze...")
    
    print("   ‚Ä¢ √âtat initial:")
    params_before = count_parameters(model)
    print(f"     - Trainable: {params_before['trainable']:,}")
    
    model.freeze_backbone()
    params_frozen = count_parameters(model)
    print(f"     - Apr√®s freeze: {params_frozen['trainable']:,}")
    
    model.unfreeze_backbone()
    params_unfrozen = count_parameters(model)
    print(f"     - Apr√®s unfreeze: {params_unfrozen['trainable']:,}")
    
    model.unfreeze_last_n_blocks(n=2)
    params_partial = count_parameters(model)
    print(f"     - Unfreeze 2 derniers blocs: {params_partial['trainable']:,}")
    
    # 6. Test sur GPU si disponible
    print("\n6Ô∏è‚É£ Test compatibilit√© GPU...")
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print("   ‚úì CUDA disponible")
    elif torch.backends.mps.is_available():
        device = torch.device('mps')
        print("   ‚úì MPS (Apple Silicon) disponible")
    else:
        device = torch.device('cpu')
        print("   ‚ö†Ô∏è Seulement CPU disponible")
    
    try:
        model = model.to(device)
        x = torch.randn(2, 8, 3, 224, 224).to(device)
        with torch.no_grad():
            output = model(x)
        print(f"   ‚úì Forward sur {device}: OK")
        print(f"   ‚Ä¢ Output device: {output.device}")
    except Exception as e:
        print(f"   ‚ùå Erreur sur {device}: {e}")
    
    # 7. V√©rifications finales
    print("\n7Ô∏è‚É£ V√©rifications:")
    print("   ‚úì Shape de sortie correcte")
    print("   ‚úì Gradient flow OK")
    print("   ‚úì Compatible GPU")
    print("   ‚úì Freeze/Unfreeze fonctionne")
    
    print("\n" + "="*70)
    print("‚úÖ TOUS LES TESTS R√âUSSIS!")
    print("="*70)
    print("\nüí° Le mod√®le est pr√™t pour l'entra√Ænement!")
    
    # 8. R√©sum√© pour le m√©moire
    print("\nüìä R√âSUM√â POUR LE M√âMOIRE:")
    print("-" * 70)
    print(f"Architecture: ResNet50 + LSTM")
    print(f"Param√®tres totaux: {params['total']:,}")
    print(f"Backbone: ResNet50 pr√©-entra√Æn√© (ImageNet)")
    print(f"LSTM: 2 couches, hidden_size=256")
    print(f"Dropout: 0.3")
    print(f"Classes: 2 (collision/normal)")
    print("-" * 70)


if __name__ == "__main__":
    test_model()