"""
VideoMAE (Hugging Face) pour Classification de Vid√©os
======================================================

Mod√®le Transformer pr√©-entra√Æn√© avec Masked Autoencoding.

Source: "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training" (2022)
Hugging Face: MCG-NJU/videomae-base-finetuned-kinetics

Architecture:
    - Vision Transformer avec MAE pre-training
    - Apprentissage auto-supervis√© sur vid√©os masqu√©es
    - Pr√©-entra√Æn√© sur Kinetics-400

Auteur: Jerome
Exp√©rience: exp_006 (Transformer MAE)
"""

import torch
import torch.nn as nn
from transformers import VideoMAEModel, VideoMAEConfig
from typing import Tuple, Dict


class VideoMAEClassifier(nn.Module):
    """
    VideoMAE pour classification de collisions.
    
    ‚ö†Ô∏è  IMPORTANT: VideoMAE n√©cessite EXACTEMENT 16 frames.
    Le mod√®le pr√©-entra√Æn√© utilise des embeddings de position fixes.
    
    Args:
        num_classes (int): Nombre de classes (2)
        pretrained_model (str): Nom du mod√®le HF
        freeze_backbone (bool): Freeze le transformer
        dropout (float): Dropout pour le classifier
        
    Input Shape:
        (batch, 16, 3, 224, 224)  ‚Üê EXACTEMENT 16 frames
        
    Output Shape:
        (batch, num_classes)
    """
    
    def __init__(
        self,
        num_classes: int = 2,
        pretrained_model: str = "MCG-NJU/videomae-base-finetuned-kinetics",
        freeze_backbone: bool = False,
        dropout: float = 0.3
    ):
        super(VideoMAEClassifier, self).__init__()
        
        self.num_classes = num_classes
        self._is_backbone_frozen = freeze_backbone
        
        # Charger le mod√®le pr√©-entra√Æn√©
        print(f"   üì• Chargement de {pretrained_model}...")
        self.videomae = VideoMAEModel.from_pretrained(pretrained_model)
        
        # Dimension des features
        self.hidden_size = self.videomae.config.hidden_size  # 768
        
        # Freeze si demand√©
        if freeze_backbone:
            for param in self.videomae.parameters():
                param.requires_grad = False
            print("   üîí Backbone freez√©")
        else:
            print("   üîì Backbone entra√Ænable")
        
        # Classifier custom
        self.classifier = nn.Sequential(
            nn.LayerNorm(self.hidden_size),
            nn.Dropout(dropout),
            nn.Linear(self.hidden_size, self.hidden_size // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(self.hidden_size // 2, num_classes)
        )
        
        print(f"‚úÖ VideoMAE initialis√©:")
        print(f"   ‚Ä¢ Model: {pretrained_model}")
        print(f"   ‚Ä¢ Hidden size: {self.hidden_size}")
        print(f"   ‚Ä¢ Dropout: {dropout}")
        print(f"   ‚Ä¢ Num classes: {num_classes}")
        print(f"   ‚ö†Ô∏è  N√âCESSITE EXACTEMENT 16 FRAMES")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass.
        
        Args:
            x: (batch, 16, 3, H, W)  ‚Üê DOIT √™tre 16 frames
            
        Returns:
            logits: (batch, num_classes)
        """
        # VideoMAE n√©cessite EXACTEMENT 16 frames
        assert x.shape[1] == 16, f"VideoMAE n√©cessite 16 frames, re√ßu {x.shape[1]}"
        
        # VideoMAE attend (batch, num_frames, channels, H, W)
        outputs = self.videomae(x)
        
        # Prendre le [CLS] token
        pooled_output = outputs.last_hidden_state[:, 0]  # (batch, hidden_size)
        
        # Classification
        logits = self.classifier(pooled_output)
        
        return logits
    
    def get_num_params(self) -> Tuple[int, int]:
        total = sum(p.numel() for p in self.parameters())
        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)
        return total, trainable
    
    def freeze_backbone(self):
        for param in self.videomae.parameters():
            param.requires_grad = False
        self._is_backbone_frozen = True
        print("üîí Backbone freez√©")
    
    def unfreeze_backbone(self):
        for param in self.videomae.parameters():
            param.requires_grad = True
        self._is_backbone_frozen = False
        print("üîì Backbone unfreez√©")
    
    def unfreeze_last_n_layers(self, n: int = 2):
        """Unfreeze les n derni√®res couches du transformer."""
        self.freeze_backbone()
        
        # VideoMAE a 12 layers
        total_layers = len(self.videomae.encoder.layer)
        start_idx = max(0, total_layers - n)
        
        for i in range(start_idx, total_layers):
            for param in self.videomae.encoder.layer[i].parameters():
                param.requires_grad = True
        
        print(f"üîì Derni√®res {n} couches unfreez√©es")


def count_parameters(model: nn.Module) -> Dict[str, int]:
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    frozen = total - trainable
    
    return {
        'total': total,
        'trainable': trainable,
        'frozen': frozen,
        'trainable_percent': (trainable / total * 100) if total > 0 else 0
    }


def test_model():
    """Test du mod√®le VideoMAE."""
    print("üß™ TEST DU MOD√àLE VideoMAE\n")
    print("="*70)
    
    # 1. Cr√©er le mod√®le
    print("\n1Ô∏è‚É£ Cr√©ation du mod√®le...")
    model = VideoMAEClassifier(
        num_classes=2,
        pretrained_model="MCG-NJU/videomae-base-finetuned-kinetics",
        freeze_backbone=False,
        dropout=0.3
    )
    
    # 2. Statistiques
    print("\n2Ô∏è‚É£ Statistiques:")
    params = count_parameters(model)
    print(f"   ‚Ä¢ Total: {params['total']:,}")
    print(f"   ‚Ä¢ Trainable: {params['trainable']:,}")
    print(f"   ‚Ä¢ Frozen: {params['frozen']:,}")
    
    # 3. Test forward
    print("\n3Ô∏è‚É£ Test forward pass...")
    batch_size = 2
    num_frames = 16  # VideoMAE pr√©f√®re 16 frames
    x = torch.randn(batch_size, num_frames, 3, 224, 224)
    print(f"   ‚Ä¢ Input: {x.shape}")
    
    model.eval()
    with torch.no_grad():
        output = model(x)
    
    print(f"   ‚Ä¢ Output: {output.shape}")
    print(f"   ‚Ä¢ Range: [{output.min():.3f}, {output.max():.3f}]")
    
    # 4. Test configurations
    print("\n4Ô∏è‚É£ Test configurations...")
    print("   ‚ö†Ô∏è  VideoMAE n√©cessite EXACTEMENT 16 frames (pr√©-entra√Ænement)")
    for nf in [16]:  # Seulement 16 frames
        x = torch.randn(1, nf, 3, 224, 224)
        with torch.no_grad():
            out = model(x)
        print(f"   ‚úì {nf} frames: {out.shape}")
    
    # 5. GPU
    print("\n5Ô∏è‚É£ Test GPU...")
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    print(f"   ‚Ä¢ Device: {device}")
    
    try:
        model = model.to(device)
        x = torch.randn(1, 16, 3, 224, 224).to(device)
        with torch.no_grad():
            out = model(x)
        print(f"   ‚úì Forward sur {device}: OK")
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")
    
    # 6. Freeze/Unfreeze
    print("\n6Ô∏è‚É£ Test freeze/unfreeze...")
    p_before = count_parameters(model)['trainable']
    print(f"   ‚Ä¢ Avant: {p_before:,}")
    
    model.freeze_backbone()
    p_frozen = count_parameters(model)['trainable']
    print(f"   ‚Ä¢ Apr√®s freeze: {p_frozen:,}")
    
    model.unfreeze_last_n_layers(2)
    p_partial = count_parameters(model)['trainable']
    print(f"   ‚Ä¢ Unfreeze 2 layers: {p_partial:,}")
    
    print("\n" + "="*70)
    print("‚úÖ TOUS LES TESTS R√âUSSIS!")
    print("="*70)
    
    # R√©sum√©
    print("\nüìä R√âSUM√â:")
    print("-" * 70)
    print(f"Architecture: VideoMAE (Masked Autoencoder)")
    print(f"Param√®tres: {params['total']:,}")
    print(f"Source: MCG-NJU/videomae-base-finetuned-kinetics")
    print(f"Pr√©-entra√Æn√©: Kinetics-400 (MAE)")
    print(f"Type: Transformer avec apprentissage auto-supervis√©")
    print(f"‚ö†Ô∏è  N√âCESSITE: Exactement 16 frames (embeddings fixes)")
    print("-" * 70)


if __name__ == "__main__":
    test_model()